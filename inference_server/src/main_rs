
use onnxruntime::{
    GraphOptimizationLevel, LoggingLevel, environment::Environment, ndarray::Array,
    session::Session,
};
use polars::prelude::*;
use rayon::prelude::*;
use std::fs::File;
use std::sync::{Arc, Mutex};

// fn load_model() -> Session<'static>{
//     let env = Environment::builder()
//         // .with_name("ONNX_GPU")
//         .with_log_level(onnxruntime::LoggingLevel::Warning)
//         .build()
//         .unwrap();

//     let session = env.new_session_builder()
//         .unwrap()
//         // .with_cuda() // Enable GPU acceleration
//         // .unwrap()
//         .with_model_from_file("onnx_models/pipeline_svc.onnx")
//         .unwrap();

//     // Arc::new(session)
//     session
// }

// fn process_chunk(model: Session, df: &DataFrame) -> Vec<f32> {
//     let features = df.to_ndarray::<Float32Type>().unwrap();

//     features.axis_chunks_iter(Axis(0), 10_000) // Process in 10K-row chunks
//         .flat_map(|batch| {
//             let input_tensor = ndarray::Array::from_shape_vec((batch.shape()[0], batch.shape()[1]), batch.to_owned().into_raw_vec()).unwrap();

//             let outputs = model.run(vec![input_tensor.into_dyn()]).unwrap();

//             let predictions: Vec<f32> = outputs[0].to_array_view::<f32>().unwrap().to_vec();
//             predictions
//         })
//         .collect()
// }

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // let model = load_model();

    // Load large dataset in chunks
    // let dataset_path = "bigdata.parquet";
    // let df = ParquetReader::new(File::open(dataset_path).unwrap()).finish().unwrap();
    let environment = Environment::builder()
        .with_name("test")
        // .with_log_level(LoggingLevel::Verbose)
        //.with_library_path("/home/chinedu/Downloads/onnxruntime-linux-x64-1.21.0/lib")
        .build()
        .unwrap();

    let session = environment
        .new_session_builder()?
        // .unwrap()
        .with_optimization_level(GraphOptimizationLevel::Basic)?
        // .unwrap()
        // .with_intra_op_num_threads(1)?
        .with_number_threads(1)?
        // .with_model_downloaded(ImageClassification::SqueezeNet)?;
        // .unwrap()
        .with_model_from_file("onnx_models/pipeline_lightgbm.onnx")?;
        // .unwrap();

    let dataset_path = "data/processed_data.csv";
    let csvfile = File::open(dataset_path).unwrap();

    let df = CsvReadOptions::default()
        .with_has_header(true)
        .try_into_reader_with_file_path(Some(dataset_path.into()))
        .unwrap()
        .finish()
        .unwrap();
    // /home/chinedu/Downloads/onnxruntime-linux-x64-1.21.0/lib
    // let df =  CsvReader::from_path("data/processed_data.csv")?
    //     .infer_schema(None)
    //     .has_header(true)
    //     .finish();

    let target_column = df.column("diabetes").unwrap().clone();

    let df = df.drop("diabetes").unwrap();
    println!("DataFrame shape: {:?}", df.shape());
    
    
    let features = df.to_ndarray::<Float32Type>(IndexOrder::Fortran).unwrap();

    // println!("Features: {:?}", features.shape());
    //let input_tensor = ndarray::Array::from_shape_vec((batch.shape()[0], batch.shape()[1]), batch.to_owned().into_raw_vec()).unwrap();
    let input_tensor = Array::from_shape_vec(
        (features.shape()[0], features.shape()[1]),
        features.iter().cloned().collect(),
    )
    .unwrap()
    .into_dyn();

    let input0_shape: Vec<usize> = session.inputs[0]
    .dimensions()
    .map(std::option::Option::unwrap)
    .collect();

    let output0_shape: Vec<usize> = session.outputs[0]
        .dimensions()
        .map(std::option::Option::unwrap)
        .collect();
    println!("Input shape: {:?}", input0_shape);
    println!("Output shape: {:?}", output0_shape);

    // let input_tensor = input_tensor.into_onnx().unwrap();
    // let outputs= session.run(input_tensor)?;
    // let outputs = session.run(vec![input_tensor])?;
    // print!("Outputs: {:?}", outputs);

    Ok(())

    // let predictions: Vec<f32> = outputs[0].to_array_view::<f32>().unwrap().to_vec();

    // let predictions = process_chunk(model, &df);

    // println!("Predictions: {:?}", predictions);
    // let chunk_size = 1_000_000; // Process 1M rows per thread
    // let num_chunks = (df.height() + chunk_size - 1) / chunk_size;

    // let data = Arc::new(Mutex::new(Vec::new()));

    // (0..num_chunks).into_par_iter().for_each(|i| {
    //     let start = i * chunk_size;
    //     let end = ((i + 1) * chunk_size).min(df.height());

    //     if let Ok(chunk) = df.slice(start, end - start) {
    //         let predictions = process_chunk(model.clone(), &chunk);
    //         data.lock().unwrap().extend(predictions);
    //     }
    // });

    // println!("Inference complete! Processed {} records.", df.height());
}
